<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width initial-scale=1">

  <title>Picking Objects</title>
  <meta name="description" content="Ein is a complete pick-and-place stack for Baxter.
">

  <link rel="stylesheet" href="/ein/css/main.css">
  <link rel="canonical" href="http://h2r.github.io/ein/pickingobjects/">
  <link rel="alternate" type="application/atom+xml" title="Ein" href="http://h2r.github.io/ein/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/ein/">Ein</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">


        
          
        
          
        
          
        
          
          <a class="page-link" href="/ein/install/">Install</a>
          
        
          
          <a class="page-link" href="/ein/getstarted/">Get Started</a>
          
        
          
          <a class="page-link" href="/ein/calibration/">Calibration</a>
          
        
          
          <a class="page-link" href="/ein/pickingobjects/">Picking Objects</a>
          
        
          
          <a class="page-link" href="/ein/scanningobjects/">Mapping Objects</a>
          
        
          
          <a class="page-link" href="/ein/language/">Back Language</a>
          
        
          
          <a class="page-link" href="/ein/movement/">Movement</a>
          
        
          
          <a class="page-link" href="/ein/words/">Word Index</a>
          
        
          
          <a class="page-link" href="/ein/faq/">FAQ</a>
          
        
          
          <a class="page-link" href="/ein/about/">About</a>
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Picking Objects</h1>
  </header>

  <article class="post-content">
    <p>Now we will use Baxter to pick things up. Restricting our picking to a
table allows us to make some assumptions about the world. In
particular, Ein assumes there is a flat ground surface parallel to the
xy-plane whose constant z coordinate is known and that gravity pulls
toward that plane.</p>

<p>A consequence of these assumptions is that we can forgo labeling grasp depth
and close the fingers when they encounter an obstruction. When moving toward an
object for a pick, stabbing our fingers outward from a pre-pick position, our
fingertips will either press the object against the tabletop and trigger a
grasp or go around the object (spoon handle, small block) and hit the table on
either side of the target, triggering a grasp. Grasping in free space without a
backing or resistor is less reliable because tipping is more likely to occur.</p>

<p>Another consequence is that most objects will come to rest on the
ground plane.  This is important because in order to see an object
correctly, Ein needs to know (or finds out in the process) how far
away from the camera it is.  Most computer vision systems use
individual RGB frames, or calculations performed on sequences of
frames and summarized with statistics, to perform their
operations. Ein captures tens to thousands of RGB frames and refocuses
the rays from those images into computed 2D photographs and performs
operations on those instead. Refocusing the light is very similar to
adjusting the focus on traditional camera, and the aperture of our
virtual camera is quite large, which means that we can achieve a very
narrow depth of field. Knowing the position of the ground plane is a
strong hint for the search problems involved and so speeds up the
process dramatically.</p>

<p>The basic workflow for object picking is:</p>

<ol>
  <li>(<a href="../calibration">Calibrate once</a>.)</li>
  <li>Create a map of the empty workspace (background map).</li>
  <li>Train object model.
    <ul>
      <li>Automatic grasp annotation or manual grasp annotation.</li>
    </ul>
  </li>
  <li>Map workspace and compute discrepancy (background subtraction).</li>
  <li>Detect and localize object(s) cascaded on discrepancy.</li>
  <li>Pick!</li>
</ol>

<h3 id="create-a-background-map">Create a background map.</h3>

<p>Now you need to create a workspace background model, which we call a
background map. Send the gripper to the home position by running
<code>goHome</code>.  The workspace should contain a square of two feet. It
should be as flat as possible. The system can accomodate altitude
changes but shadows and occlusion induce variance in the maps and
require careful sampling for accurate results.  (If you would like to
change the home position, follow <a href="../movement/#changing-the-home-position">these
instructions</a>.)</p>

<p>The wrist should now be pointing straight down with the camera should
be about 38 cm from the table.  If it is substantially higher or lower
and this is the same space in which you calibrated, you should reset
your table height and make sure you saved your calibration.</p>

<p>As usual, unless otherwise stated, leave the grippers in open position.</p>

<p>After making sure any object that can move is free from the workspace
and that the lighting is how you will want it during picking, issue</p>

<pre><code>tableUpdateBg
</code></pre>

<p>The arm should move in a spiral, return to its starting position, your
CPU should go under load, and a few seconds after the CPU relaxes the
formed image should show up first in the Observed Map window and next
in the Background window. The table and objects shorter than a few
centimeters should appear crisp up to the resolution of the map, which
is 1mm to a pixel at this height with a good calibration. Object
taller than a few centimeters will start to blur due to defocus.</p>

<h4 id="exercise--inspect-your-background-map">Exercise:  Inspect your background map.</h4>

<p>Zoom in and see what detail you can make out.  On a well-calibrated
Baxter, you should be able to see detail up to several millimeters.
Some examples from our lab appear below.</p>

<p><img src="../assets/bg.jpg" alt="Background map" /></p>

<p>When picking objects, you can change the gripper gap and retake the
gripper mask by pointing the arm at magic paper as during calibration
and running:
<code>
setGripperMaskWithMotion
</code></p>

<p>Here is an example of a background map with an incorrect gripper mask;
note the dark gripper pixels smeared on the lower part of the image.</p>

<p><img src="../assets/bg_badgrippermask.jpg" alt="Background map with bad gripper mask" /></p>

<h3 id="update-the-observed-map">Update the observed map.</h3>

<p>Next update the observed map. First, place an object into the robot’s field of view. Run <code>tableTakeScene</code> to create a map by
moving the arm in a spiral pattern.  This map will be rendered in the
Gaussian Map Observed View” window shortly after the arm stops moving.
Once you have created an observed map and background map, the
discrepancy views will also populate, showing differences between the
observed and background maps.  Here is an example showing the Allen
wrench from the Baxter gripper toolkit.  Note that the discrepancy
density view is nearly all black except for the well-segmented
wrench. It may help to overlay the observed map and discrepancy
density windows and alt-tab between them to find differences.  If you
do not see a well-segmented object, check your calibration and verify
that the lighting hasn’t changed since you made your background map.</p>

<p><img src="../assets/allenwrench_observed.jpg" width="350" alt="Observed map with allen wrench." />
<img src="../assets/allenwrench_density.jpg" width="350" alt="Observed map with ." /></p>

<h4 id="exercise-put-several-objects-in-the-scene-and-update-the-observed-map">Exercise: Put several objects in the scene and update the observed map.</h4>

<p>Inspect the observed map, background map, and discrepancy view.  If
     the objects do not appear well-segmented in the discrepancy
     window, than there may be a problem with your calibration.</p>

<h4 id="exercise-remove-all-objects-from-the-scene-and-update-the-observed-map--verify-the-discrepancy-is-empty">Exercise: Remove all objects from the scene and update the observed map.  Verify the discrepancy is empty.</h4>

<p>Below is an example of an observed map and discrepancy view taken
immediately after making a background map.</p>

<p><img src="../assets/bg_observed.jpg" width="350" alt="Observed map with no objects." />
<img src="../assets/bg_discrepancy.jpg" width="350" alt="Discrepancy with no objects." /></p>

<h3 id="pick-a-known-object">Pick a known object.</h3>

<p>We will first demonstrate picking with a pre-trained model from the
Rethink Baxter kit, the narrow long finger.  Make sure the gripper gap
is at its narrowest setting. Equip the rectangular rubber tips; we
will be trying for a flush grasp.  Find the narrow long finger from a
Baxter parallel gripper kit and place it in the workspace.  Here is a
picture of the setup:</p>

<p><img src="../assets/narrow_finger_scene.jpg" alt="Narrow Long Finger" /></p>

<p>Issue:</p>

<pre><code>endArgs "baxterNarrowLongFinger" setClassLabels
</code></pre>

<p>and check terminal and console output to verify the model loads.
Next, make a map of the object by running:</p>

<pre><code>tableMapBestClass
</code></pre>

<p>The arm should move in a spiral pattern and create a map of the table,
then run the detector for the narrow long finger.  You should compare
the predicted and observed maps to assess the accuracy of the pose
estimation.</p>

<p>Finally to pick the object run: 
<code>
"baxterNarrowLongFinger" deliverObject
</code></p>

<p>This word grasps the object and stops.  You must run additional
commands to move the object after the grasp, such as
<code>assumeHandingPose</code>.</p>

<h4 id="exercise-configurations">Exercise: Configurations</h4>

<p>The model for baxterNarrowLongFinger only contains one configuration
of the finger, lying on its side.  Try placing the finger in other
configurations and assess the detector performance by trying to pick.
For example, place it vertically or try placing it on the opposite
orientation on its side.  Try to determine the configuration in which
we have created a model.</p>

<p>For us, the model picks reasonably well in any configuration because
it lines up the long part of the finger, but the likelihood and pose
estimates are much better with the correct configuration.</p>

<h4 id="exercise-dribble">Exercise: Dribble</h4>

<p>Write a program to pick the object, perturb position, and place it.
The word <code>perturbPosition</code> moves the arm a random position and
orientation from its current location.  The word <code>touchDown</code> moves the
arm to touch the table.  Run it 100 times.</p>

<p>Ein includes a word to do this task in a loop:</p>

<pre><code>tableInfiniteDribbleBest
</code></pre>

<p>Compare your implementation to ours.</p>

<h3 id="make-a-model-for-your-object">Make a model for your object.</h3>

<p>But you probably want to do more than that one object!  Next, find an
object you want to pick, and make sure it fits in the grippers.  Place
the object at the center of the workspace and return the arm as well
with <code>goHome</code>.</p>

<p>Place the object under the end effector, clear the space of any other
object, and run</p>

<pre><code>tableQuickScan
</code></pre>

<p>This generates an object model capable of localizing the object once
moved and rotated within the plane of the table. Now you need to
annotate a grasp. Issue</p>

<pre><code>tableLock3dGrasp
</code></pre>
<p>and wait for it to finish. Once the stack is clear, by whatever means you choose
and without touching or moving the object from its locked position, drive the arm to a position which will result in a valid
grasp when the fingers close. Recall that the arm will move to a pre-pick position and advance towards the object until it encounters
resistence, at which point the grippers will close. Keep this in mind when choosing a grasp. If you move the object, <code>clearStacks</code> and 
start over from <code>tableLock3dGrasp</code>.</p>

<p>Issue</p>

<pre><code>clearClass3dGrasps
</code></pre>
<p>to clear the grasps for this object, removing any default or previously
assigned grasps. If you add a bad grasp or knock the object and have to start
over, make sure to clear your grasps.</p>

<p>When you are ready to add the current pose as a grasp, issue</p>

<pre><code>add3dGrasp
</code></pre>
<p>You can and should add multiple grasps so that, if the first grasp or pre-grasp position is infeasible, Ein can select
an alternative grasp.</p>

<p>Issue</p>

<pre><code>writeFocusedClass
</code></pre>

<p>The output folder will appear on the console is an autogenerated name
that includes the robot’s serial number and timestamp and will appear
on the console.  You can rename the focused class by issuing:</p>

<pre><code>"newName" renameFocusedClass
</code></pre>

<p>Now it is time to test the object. Return to your workspace center and run</p>

<pre><code>tableInfiniteDribbleBest
</code></pre>

<p>Dribbling is a nice demo, but you probably want the capability to
pause the arm when it holds the object and command its movement for
your own purposes.  First map the area with:</p>

<pre><code>tableMapBestClass
</code></pre>

<p>This word maps the area and adds a detection to the list of Blue Boxes
visible in the Ein main window.</p>

<pre><code>setPlaceModeToHand
</code></pre>

<p>changes the place mode to handing, which is the default, but some
other Ein program may have changed it so make sure its value is set
properly.</p>

<p>Now, when you issue</p>

<pre><code>"itemA" deliverObject
</code></pre>

<p>the arm will move to pick the object and stop with an empty call stack after the attempt.
You are free to command the arm from this point.</p>

<p>To clear all mapped Blue Box detections, issue</p>

<pre><code>clearBlueBoxMemories
</code></pre>

<p>Note that this does not clear the predicted scene objects which show up in the Predicted View. To do that, issue</p>

<pre><code>sceneClearPredictedObjects tableUpdateMaps
</code></pre>

<p>Finally there is the matter of handling multiple objects over a larger area. Defining scanning patterns is beyond
the scope of this page, but defining multiple objects and detecting them across multiple workspaces is not.</p>

<p>Train a second object model and issue</p>

<pre><code>endArgs "itemA" "itemB" setClassLabels
</code></pre>

<p>Train workspaces a and b, save their poses. It’s ok if the workspaces overlap. Put one object in each workspace, make sure it is
closer to the center of its workspace than the other object. For each workspace, move the arm, load the background model, detect best, add
to blue boxes, can command which blue box to pick.</p>

<h4 id="summary--training-and-dribbling-an-object">Summary:  Training and Dribbling an Object.</h4>

<p>Here is the REPL history for a session that</p>

<pre><code>tableUpdateBg
tableQuickScan
tableLock3dGrasp
clearClass3dGrasps
zDown 
zDown 
...
xUp
yUp
zDown
add3dGrasp
writeFocusedClass
tableReset
endArgs "catScan5_011509P0027right2016-07-02_13:03:38/autoClass_011509P0027_right_2016-07-02_13:19:18" setClassLabels
tableInfiniteDribbleBest
</code></pre>

<h4 id="exercise--object-delivery">Exercise:  Object delivery.</h4>

<p>Write a program to pick up the object and hand it to the person.</p>

<h4 id="exercise--clean-up">Exercise:  Clean up.</h4>

<p>Write a program to pick up both objects and move them to the back
workspace.</p>

<h4 id="exercise--drop">Exercise:  Drop.</h4>

<p>Write a program to pick up an object and drop it from a height of
10cm.</p>


  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Ein</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
<!--        <ul class="contact-list">
          <li>Ein</li>
          <li><a href="mailto:"></a></li>
        </ul>-->
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/h2r">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">h2r</span>
            </a>
          </li>
          

          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">Ein is a complete pick-and-place stack for Baxter.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
